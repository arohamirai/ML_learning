{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通用代码框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 必要参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset_path = \"./image/train/\"\n",
    "testset_path = \"./image/test/\"\n",
    "trainset_record_path = \"./output/training-images/\"\n",
    "validset_record_path = \"./output/validing-images/\"\n",
    "testset_record_path = \"./output/testing-images/\"\n",
    "log_dir = \"./logdir/\"\n",
    "ckpt_path = \"./checkpoint/\"\n",
    "\n",
    "image_width = 24\n",
    "image_height = 24\n",
    "image_channels = 3\n",
    "\n",
    "#总共3类\n",
    "num_breed = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 辅助函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义w,b初始化函数\n",
    "def variable_with_weight_loss(shape, stddev, wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 通用框架函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(X):\n",
    "    with tf.name_scope(\"input\") as scope:\n",
    "        #x = tf.placeholder(tf.float32,shape = [None, image_height, image_width, image_channels], name = \"x_input\")\n",
    "        #y = tf.placeholder(tf.int64,shape = [X.shape[0]],name = \"y_input\")\n",
    "        #x = image_batch\n",
    "        #y = label_batch\n",
    "        x = X\n",
    "        tf.summary.image(\"image_input\",X, batch_size)\n",
    "\n",
    "    #卷积层一\n",
    "    with tf.name_scope(\"conv1\") as scope:\n",
    "        weight1 = variable_with_weight_loss(shape=[5, 5, 3, 64], stddev=5e-2, wl=0.0)\n",
    "        kernel1 = tf.nn.conv2d(x, weight1, [1, 1, 1, 1], padding='SAME')\n",
    "        bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    \n",
    "        tf.summary.histogram(\"weight1\",weight1)\n",
    "    \n",
    "    #卷积层二\n",
    "    with tf.name_scope(\"conv2\") as scope:\n",
    "        weight2 = variable_with_weight_loss(shape=[5, 5, 64, 64], stddev=5e-2, wl=0.0)\n",
    "        kernel2 = tf.nn.conv2d(norm1, weight2, [1, 1, 1, 1], padding='SAME')\n",
    "        bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "        conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "        norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "        tf.summary.histogram(\"weight2\",weight2)\n",
    "\n",
    "    #全连接层一\n",
    "    with tf.name_scope(\"full_connect1\") as scope:\n",
    "        reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "        weight3 = variable_with_weight_loss(shape=[6*6*64, 384], stddev=0.04, wl=0.004)\n",
    "        bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)\n",
    "    \n",
    "        tf.summary.histogram(\"weight3\",weight3)\n",
    "    \n",
    "    #全连接层二\n",
    "    with tf.name_scope(\"full_connect2\") as scope:\n",
    "        weight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, wl=0.004)\n",
    "        bias4 = tf.Variable(tf.constant(0.1, shape=[192]))                                      \n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\n",
    "    \n",
    "        tf.summary.histogram(\"weight4\",weight4)\n",
    "    \n",
    "    #全连接层三\n",
    "    with tf.name_scope(\"full_connect3\") as scope:\n",
    "        weight5 = variable_with_weight_loss(shape=[192, 3], stddev=1/192.0, wl=0.0)\n",
    "        bias5 = tf.Variable(tf.constant(0.0, shape=[3]))\n",
    "        logits = tf.add(tf.matmul(local4, weight5), bias5)\n",
    "    \n",
    "        tf.summary.histogram(\"weight5\",weight5)\n",
    "        # softmax处理\n",
    "        #Y_ = tf.nn.softmax(logits)\n",
    "        return logits\n",
    "\n",
    "def loss(X, Y):\n",
    "    logits = inference(X)\n",
    "    with tf.name_scope(\"cross_entropy\") as scope:\n",
    "        #定义训练代价函数\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "        tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "        \n",
    "    return cross_entropy\n",
    "\n",
    "def inputs(data_dir, batch_size, shuffle = True):\n",
    "    filenames =tf.train.match_filenames_once(data_dir+\"*.tfrecords\")\n",
    "    filenames_queue = tf.train.string_input_producer(filenames, shuffle = True)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filenames_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features = {\n",
    "            \"label\":tf.FixedLenFeature([],tf.int64),\n",
    "            \"image_raw\":tf.FixedLenFeature([],tf.string)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 解码图像数据\n",
    "    im = tf.decode_raw(features[\"image_raw\"],tf.uint8) \n",
    "    reshape = tf.reshape(im,(image_height,image_width,image_channels))\n",
    "    image = tf.cast(reshape,tf.float32)\n",
    "    label = tf.cast(features[\"label\"],tf.int64)\n",
    "\n",
    "\n",
    "    # 组合训练数据\n",
    "    min_after_dequeue = 100*batch_size\n",
    "    capacity = min_after_dequeue + 3*batch_size\n",
    "\n",
    "    image_batch, label_batch = tf.train.shuffle_batch(\n",
    "    (image,label),batch_size = batch_size,\n",
    "    capacity = capacity,min_after_dequeue = min_after_dequeue\n",
    "    )\n",
    "    return image_batch, label_batch\n",
    "\n",
    "\n",
    "def train(total_loss):\n",
    "    # 定义梯度优化算法\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = tf.train.AdamOptimizer(1e-4).minimize(total_loss, global_step = global_step)\n",
    "    return train_op\n",
    "\n",
    "def evaluate(sess, X, Y):\n",
    "    Y_ = inference(X)\n",
    "    correct_prediction = tf.equal(tf.argmax(Y_,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype = tf.float32))\n",
    "    \n",
    "    return sess.run([accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义数据流图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "X, Y = inputs(trainset_record_path, batch_size)\n",
    "total_loss = loss(X, Y)\n",
    "train_op = train(total_loss)\n",
    "\n",
    "# 须在数据流图定义完后在merge_all\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "#在迭代控制中,记得添加tf.initialize_local_variables(),官网教程没有说明,但是如过不加，会出错\n",
    "init = [tf.global_variables_initializer(),tf.local_variables_initializer()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training......\n",
      "\n",
      "0 1.40794\n",
      "100 0.835881\n",
      "200 0.641825\n",
      "300 0.333636\n",
      "400 0.244037\n",
      "500 0.064397\n",
      "600 0.00418409\n",
      "700 0.00477062\n",
      "800 0.00688567\n",
      "900 0.00301323\n",
      "1000 0.00152629\n",
      "1100 0.00108007\n",
      "1200 0.0013108\n",
      "1300 0.000406786\n",
      "1400 0.000935769\n",
      "1500 0.000275467\n",
      "1600 0.000725677\n",
      "1700 0.000449241\n",
      "1800 0.000287554\n",
      "1900 0.00019782\n",
      "2000 0.000185532\n",
      "2100 0.000185026\n",
      "2200 0.000173659\n",
      "2300 0.000104061\n",
      "2400 0.000114449\n",
      "2500 8.12437e-05\n",
      "2600 5.23779e-05\n",
      "2700 7.05063e-05\n",
      "2800 0.0001757\n",
      "2900 5.18061e-05\n",
      "3000 7.84466e-05\n",
      "3100 1.97883e-05\n",
      "3200 7.54767e-05\n",
      "3300 3.89909e-05\n",
      "3400 2.77033e-05\n",
      "3500 2.4008e-05\n",
      "3600 9.91807e-06\n",
      "3700 2.49972e-05\n",
      "3800 1.92634e-05\n",
      "3900 8.72602e-06\n",
      "finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_iter = 4000\n",
    "init_step = 0\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "        \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess = sess, coord = coord) \n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)    \n",
    "    \n",
    "     ### 从检查点恢复训练\n",
    "    init_step = 0\n",
    "    # 验证之前是否已经保存了检查点文件\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        # 从检查点中恢复模型参数\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "        init_step = int(ckpt.model_checkpoint_path.rsplit(\"-\",1)[1])\n",
    "    \n",
    "    #训练    \n",
    "    print(\"start training......\\n\")\n",
    "    for step in range(init_step, num_iter):\n",
    "        sess.run(train_op)\n",
    "        #print(step)\n",
    "        # 每隔一定迭代步数参看loss情况,并保存检查点\n",
    "        if step % 100 == 0:\n",
    "            #loss_value  = sess.run([total_loss])\n",
    "            summary, loss_value  = sess.run([merged, total_loss])\n",
    "            summary_writer.add_summary(summary, step)\n",
    "            print(step,loss_value)\n",
    "        \n",
    "        # 每隔3000 步保存一次\n",
    "        if step % 3000 == 0:\n",
    "            saver.save(sess,ckpt_path+ \"my-model\",global_step = step)\n",
    "    \n",
    "    saver.save(sess, ckpt_path+ \"my-model\" ,global_step = num_iter)\n",
    "    summary_writer.close()\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    print(\"finished.\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

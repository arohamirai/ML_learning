{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 、文件位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_path = \"./image/train\"\n",
    "testset_path = \"./image/test\"\n",
    "output_path = \"./output\"\n",
    "\n",
    "image_width = 24\n",
    "image_height = 24\n",
    "image_channels = 3\n",
    "\n",
    "#总共3类\n",
    "num_breed = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、创建 TFRecords文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from itertools import groupby\n",
    "from scipy import misc\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class 信息\n",
    "breeds_with_path = glob.glob(trainset_path + \"/*\")\n",
    "breeds = list(map(lambda x:x.split(\"/\")[3],breeds_with_path))\n",
    "breeds_dict = dict(zip(breeds, [x for x in range(len(breeds))]))\n",
    "#print(breeds_with_path,breeds_dict,[x for x in range(len(breeds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_record(dataset_path,output_location):\n",
    "    # 获取数据集下所有文件路径\n",
    "    dataset_files = glob.glob(dataset_path + \"/*/*.jpg\")\n",
    "    # 分离出claess信息,得到（class,file_path）对的列表\n",
    "    image_filepath_with_breed = list(map(lambda file_path:(file_path.split(\"/\")[3],file_path),dataset_files))\n",
    "    \n",
    "    #每个TFRecord文件保存10个图片\n",
    "    nums_per_file = 10\n",
    "    writer = None\n",
    "    \n",
    "    # 记录当前读到的图像index\n",
    "    current_index = 0\n",
    "    for breed, file_path in image_filepath_with_breed:\n",
    "        if current_index % nums_per_file == 0:\n",
    "            if writer:\n",
    "                print(\"current_index = \",current_index,\"\\n\")\n",
    "                writer.close()\n",
    "            \n",
    "            # 格式化字符串\n",
    "            record_filename = \"{output_location}{current_index}.tfrecords\".format(\n",
    "            output_location = output_location,\n",
    "            current_index = current_index)\n",
    "            \n",
    "            writer = tf.python_io.TFRecordWriter(record_filename)\n",
    "            \n",
    "        current_index += 1\n",
    "        \n",
    "        # 读取图像并转换为tf.example格式\n",
    "        image = misc.imread(file_path)\n",
    "            \n",
    "        # 图像转换为Byte型\n",
    "        image_raw = image.tobytes()\n",
    "        \n",
    "        example = tf.train.Example(features = \n",
    "                                  tf.train.Features(feature = {\n",
    "                    \"label\": tf.train.Feature(int64_list = tf.train.Int64List(value = [breeds_dict[breed]])),\n",
    "                    \"image_raw\": tf.train.Feature(bytes_list = tf.train.BytesList(value = [image_raw]))   \n",
    "                }     \n",
    "            )    \n",
    "        )\n",
    "        \n",
    "        # 写入文件\n",
    "        writer.write(example.SerializeToString())\n",
    "        \n",
    "    # 关闭最后一个文件\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create_record(trainset_path,output_path + \"/training-images/\")\n",
    "#create_record(testset_path,output_path + \"/testing-images/\")\n",
    "print(\"finished\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、 读取TFRecord 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames =tf.train.match_filenames_once(output_path + \"/training-images/*.tfrecords\")\n",
    "#[\"/home/lile/ML_learning/ML_framework/output/training-images/10.tfrecords\",\"/home/lile/ML_learning/ML_framework/output/training-images/0.tfrecords\"] #\n",
    "filenames_queue = tf.train.string_input_producer(filenames, shuffle = True)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filenames_queue)\n",
    "features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features = {\n",
    "        \"label\":tf.FixedLenFeature([],tf.int64),\n",
    "        \"image_raw\":tf.FixedLenFeature([],tf.string)\n",
    "    }\n",
    ")\n",
    "\n",
    "# 解码图像数据\n",
    "im = tf.decode_raw(features[\"image_raw\"],tf.uint8) \n",
    "reshape = tf.reshape(im,(image_height,image_width,image_channels))\n",
    "image = tf.cast(reshape,tf.float32)\n",
    "label = tf.cast(features[\"label\"],tf.int64)\n",
    "\n",
    "\n",
    "# 组合训练数据\n",
    "batch_size = 5\n",
    "min_after_dequeue = 100*batch_size\n",
    "capacity = min_after_dequeue + 3*batch_size\n",
    "\n",
    "image_batch, label_batch = tf.train.shuffle_batch(\n",
    "(image,label),batch_size = batch_size,\n",
    "capacity = capacity,min_after_dequeue = min_after_dequeue\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、定义网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### 各层图片尺寸 \n",
    "# input   [batch_size, 224, 224, 3]\n",
    "# conv1   [32, 56, 56, 64]\n",
    "# pool1   [32, 27, 27, 64]\n",
    "# conv2   [32, 27, 27, 192]\n",
    "# pool2   [32, 13, 13, 192]\n",
    "# conv3   [32, 13, 13, 384]\n",
    "# conv4   [32, 13, 13, 256]\n",
    "# conv5   [32, 13, 13, 256]\n",
    "# pool5   [32, 6, 6, 256]\n",
    "\n",
    "\n",
    "# 定义w,b初始化函数\n",
    "def variable_with_weight_loss(shape, stddev, wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var\n",
    "\n",
    "\n",
    "#def interface(x):\n",
    "    # 输入层\n",
    "with tf.name_scope(\"input\") as scope:\n",
    "    x = tf.placeholder(tf.float32,shape = [None, image_height, image_width, image_channels], name = \"x_input\")\n",
    "    y = tf.placeholder(tf.int64,shape = [batch_size],name = \"y_input\")\n",
    "    #x = image_batch\n",
    "    #y = label_batch\n",
    "    tf.summary.image(\"image_input\",x, batch_size)\n",
    "\n",
    "    #卷积层一\n",
    "with tf.name_scope(\"conv1\") as scope:\n",
    "    weight1 = variable_with_weight_loss(shape=[5, 5, 3, 64], stddev=5e-2, wl=0.0)\n",
    "    kernel1 = tf.nn.conv2d(x, weight1, [1, 1, 1, 1], padding='SAME')\n",
    "    bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "    conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\",weight1)\n",
    "    \n",
    "    #卷积层二\n",
    "with tf.name_scope(\"conv2\") as scope:\n",
    "    weight2 = variable_with_weight_loss(shape=[5, 5, 64, 64], stddev=5e-2, wl=0.0)\n",
    "    kernel2 = tf.nn.conv2d(norm1, weight2, [1, 1, 1, 1], padding='SAME')\n",
    "    bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\",weight2)\n",
    "\n",
    "    #全连接层一\n",
    "with tf.name_scope(\"full_connect1\") as scope:\n",
    "    reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "    weight3 = variable_with_weight_loss(shape=[6*6*64, 384], stddev=0.04, wl=0.004)\n",
    "    bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)\n",
    "    \n",
    "    tf.summary.histogram(\"weight3\",weight3)\n",
    "    \n",
    "    #全连接层二\n",
    "with tf.name_scope(\"full_connect2\") as scope:\n",
    "    weight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, wl=0.004)\n",
    "    bias4 = tf.Variable(tf.constant(0.1, shape=[192]))                                      \n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\n",
    "    \n",
    "    tf.summary.histogram(\"weight4\",weight4)\n",
    "    \n",
    "    #全连接层三\n",
    "with tf.name_scope(\"full_connect3\") as scope:\n",
    "    weight5 = variable_with_weight_loss(shape=[192, 3], stddev=1/192.0, wl=0.0)\n",
    "    bias5 = tf.Variable(tf.constant(0.0, shape=[3]))\n",
    "    logits = tf.add(tf.matmul(local4, weight5), bias5)\n",
    "    \n",
    "    tf.summary.histogram(\"weight5\",weight5)\n",
    "    # softmax处理\n",
    "    #y_ = tf.nn.softmax(fc3)\n",
    "    \n",
    "with tf.name_scope(\"cross_entropy\") as scope:\n",
    "    # 定义训练代价函数\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "    \n",
    "#cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "# 定义梯度优化算法\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# 评估精确度\n",
    "#correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 汇总数据\n",
    "log_dir = \"./logdir\"\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "#test_writer = tf.summary.FileWriter(log_dir + '/test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training......\n",
      "\n",
      "0 0.75729\n",
      "100 1.0422\n",
      "200 0.747515\n",
      "300 0.757535\n",
      "400 0.197599\n",
      "500 0.237668\n",
      "600 0.133833\n",
      "700 0.0164611\n",
      "800 0.0604504\n",
      "900 0.00558925\n",
      "1000 0.00627413\n",
      "1100 0.000683495\n",
      "1200 0.00213882\n",
      "1300 0.000604504\n",
      "1400 0.00168171\n",
      "1500 0.000577123\n",
      "1600 0.000879515\n",
      "1700 0.00034235\n",
      "1800 0.00116273\n",
      "1900 0.00032395\n",
      "2000 0.000530722\n",
      "2100 0.000456709\n",
      "2200 0.000521394\n",
      "2300 0.000284724\n",
      "2400 0.000694109\n",
      "2500 0.000294472\n",
      "2600 0.000223545\n",
      "2700 0.000106802\n",
      "2800 0.000111117\n",
      "2900 0.000135625\n",
      "stop training.\n",
      "\n",
      "finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_iter = 3000\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = [tf.global_variables_initializer(),tf.local_variables_initializer()]\n",
    "with tf.Session() as sess:\n",
    "    #在迭代控制中,记得添加tf.initialize_local_variables(),官网教程没有说明,但是如过不加，会出错\n",
    "    sess.run(init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "    #训练    \n",
    "    print(\"start training......\\n\")\n",
    "    for step in range(num_iter):\n",
    "        images_batch,labels_batch = sess.run([image_batch,label_batch])\n",
    "        \n",
    "        _ = sess.run(train_step, feed_dict = {x:images_batch,y:labels_batch})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            summary, loss_value  = sess.run([merged, cross_entropy],feed_dict = {x:images_batch,y:labels_batch})\n",
    "            #format_str = ('step %d, loss = %.2f')\n",
    "            print(step,loss_value)\n",
    "            train_writer.add_summary(summary, step)\n",
    "            saver.save(sess,\"./checkpoint/my-model\",global_step = step)\n",
    "            \n",
    "    train_writer.close()\n",
    "     saver.save(sess,\"./checkpoint/my-model\",global_step = num_iter)\n",
    "    print(\"stop training.\\n\")\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    print(\"finished.\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
